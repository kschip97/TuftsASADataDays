---
title: "Data Days Model Building, Visualization, and Management"
author: "Kees Schipper"
date: "3/15/2021"
output: 
  html_document:
    toc: True
    toc_float: True
    toc_depth: 3
    highlight: zenburn
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(gapminder)
library(sjPlot)
library(broom)
```

```{css, echo=FALSE}
pre, code {white-space:pre !important; overflow-x:auto}
```

# Getting our data

```{r GetData}
df <- gapminder %>%
  mutate(gdp1000 = gdpPercap/1000,
         pop100k = pop/100000)
head(df)
```

### Taking a look at our data characteristics

```{r SummaryData}
summary(df)
```

```{r}
unique(df$country) #lot of countries, let's visualize
```

### visualizing our data

```{r VizCountries}

df %>%
  ggplot(aes(x = year, y = lifeExp)) +
  geom_line(aes(color = continent, group = country)) +
  labs(title = "Gapminder Life Expectancy By Year")

```


```{r SplitViz}

df %>%
  ggplot(aes(x = year, y = lifeExp)) +
  geom_line(aes(color = continent, group = country)) +
  geom_smooth(se = T, method = 'loess') +
  facet_wrap(~continent) +
  labs(title = "Gapminder Life Expectancy By Year")

```

# Basic Modelling

```{r}
names(df)
```


Let's create a basic model of life expectancy, predicted by GDP per capita, year, and population. But let's start with just Asia

```{r}
# keep only countries within the continent of Asia
mod1_df <- df %>%
  filter(continent == "Asia")

# formula outcome ~ pred1 + pred2 + pred3

mod1 <- lm(lifeExp ~ year + pop100k + gdp1000, data = mod1_df)
# can summarise our model with the summary function
summary(mod1)

```

## Model Assumptions

We see that year and gdpPercap is significant. We should also check for normality of residuals, heteroscedasticity, etc... Which we can do by just calling the plot function on our model

```{r ModDiagnostics}
par(mfrow = c(2, 2))
plot(mod1)

```

qq looks somewhat normal, residuals vs fitted look all right, not a lot of leverage points... alright we're not being too picky here.

---

We can also look at component plus residual plots of our data to check linearity of our predictors with respect to the outcome (this is only useful for a multivariate linear model)

```{r CRplots}
library(car)
car::crPlots(mod1)

```


## Improving our model

Here we can see that `gdp1000` is not necessarily linear, and population might stray from linear as well. We might want to amend this in our model. However population by itself wasn't significant, so we'll remove that from our model

```{r Model2}
mod2 <- lm(lifeExp ~ year + gdp1000 + I(gdp1000**2) + pop100k, data = mod1_df)
# the I() tells the model to treat the gdpPercap term 'as is,' and not to adjust it or ignore it
summary(mod2)
```

If we want to check to see if our model has improved, we can use the anova function to conduct a goodness of fit F-test between two models. The below anova is testing if mod2 fits better than mod1. If p < 0.05 the improvement in fit is statistically significant

### Checking model fit

```{r CheckModFit}
# let's check our model's fit from the first model. The below code
# performs an F-test
anova(mod1, mod2)
```


# The Broom package

The summary function is great, but it's hard to work with the output of it directly. To do this, a package was developed called `broom` which allows you to fortify your data with fitted values, create a data frame with your model output, and get goodness of fit statistics

broom has 3 very useful functions called `tidy`, `augment`, and `glance`. `tidy` returns the same data as your summary table (and more) in the form of a data frame. You can also ask for confidence intervals, p values, and exponentiation, among other things, depending on the model you've fit

### `tidy`

```{r tidy}
tidy(mod2, conf.int = T, statistic = F, std.error = T, p.value = T, conf.level = 0.95)
```

Augment returns a data frame of fitted values for your data, which you can then use to fit your trend line over your data. However, because we have a multivariable model, a trend line doesn't fit nicely to any one variable. I'll make a univariate model so this will make more sense.

### `augment`

```{r augment}

mod3 <- lm(lifeExp ~ year, data = mod1_df)
augdata <- augment(mod3, interval = "prediction")

augdata %>%
  ggplot(aes(x = year)) +
  geom_point(aes(y = lifeExp)) +
  geom_line(aes(y = .fitted), col = 'red') +
  geom_line(aes(y = .upper), col = 'blue') +
  geom_line(aes(y = .lower), col = 'blue') +
  geom_smooth(aes(y = lifeExp), se = F, color = 'green') +
  labs(title = "LM of LifeExpectancy vs. Year")

```

Side note: I will also mention that `augment` has some drawbacks. If you are using a poisson or negative binomial distribution to fit your data, augment returns fitted values in their non-exponentiated form. An alternative to augment is to get values from the model object itself using the `$` operator. Remember, your model output is simply a list, so you can access elements from it like any other list.

```{r GetModElsList}
mod1$coefficients
summary(mod1$residuals)
summary(mod1$fitted.values)
```



Back to the above visual...As you can see, above we have another test of our fit. We have out fitted line in red, prediction intervals, which we got from the augmented data frame in blue, and a lowess smoother in green, showing slight deviations from linearity about our model. In all honesty **you should probably plot a lowess smoother over your data before you make a linear model to check if the relationship is linear**. A one shortcut for doing that is to plot a geom_smooth of your data using `method = 'loess'`, and another `geom_smooth` using `method = lm`, which will plot a loess smoother and a fitted line, respectively. Another shortcut for doing that is the below code `pairs.panels()` from the psych package, which plots a scatterplot and correlation matrix of all of selected variables from your data frame

### Panel Plots

```{r PanelPlots}
mod1_df %>%
  dplyr::select(-continent, -country, -pop, -gdpPercap) %>%
  psych::pairs.panels(breaks = 40)
```

### `glance`

Finally, we have glance, which gives us goodness of fit statistics for our models
```{r glance, style = "max-width: 1000 px;"}
knitr::kable(glance(mod1))
knitr::kable(glance(mod2))
```

# Comparing models side-by-side

Sometimes we want to have our model in a neat output table, and we don't want to format the table every time we change our model. We can programmatically make model tables with the `tab_model` function in sjPlot

### `tab_model`

```{r TabModel}
# library(sjPlot)
sjPlot::tab_model(mod1, mod2, mod3, show.intercept = FALSE, show.ci = 0.95, show.se = F, show.p = T,
                  show.r2 = TRUE, show.fstat = T, show.aic = T, show.obs = T, rm.terms = NULL, order.terms = NULL,
                  title = "Comparison of Gapminder Models")

```

Now we can compare models programmatically without having to export the models into an Excel table every time we run our code! This function works with most of R's built-in modelling functions. There's another variant called `plot_model` which will plot estimates of our models together

### `plot_models`

```{r PlotModel}

sjPlot::plot_models(mod1, mod2, mod3, show.values = T, colors = "Dark2", ci.lvl = 0.99, axis.lim = c(-0.5, 0.5),
                    dot.size = 2, show.p = TRUE)

```
obviously, this plot is a little underwhelming because we have a very simple model, and out predictors don't show great amounts of spread from zero, but you could see how this type of plot could be useful for logistic, poisson, negative binomial, or other types of regressions with much wider confidence intervals.

---

# Many Many Many Models with purrr

So we've made a model for one continent...What if we wanted to make a model for all of the continents in our data set? Should we go through that same workflow for every one? There is a faster way, but it's somewhat tricky, so I'll present it here and leave it up to your discretion. Often times if you code by brute force method, it can take longer to do, and be many more lines, but the code may be more understandable in the end. I'll show the programmatic way of making many models though.


### Nesting in purrr

R has a package called purrr, which allows the user to nest data within a column to make a new data set. purrr also implements a programmatic concept called mapping, which is simply applying a function to every element of a list. The combination of nesting a data frame into a column, and mapping a function onto that column will allow us to efficiently make many models (though the model specifications have to be the same across groups). Here's code on how to nest

```{r nesting}
nest_df <- df %>%
  group_by(continent) %>%
  nest()

head(nest_df)
```

It's as easy as that! First you group by the category that you want to model across. In this case we want to apply similar models across continents. Now we have to make a function for a model which we can apply across the nested data frames. First I'll show you what's contained in the data column

```{r InTheNest}
head(nest_df$data[[1]])
tail(nest_df$data[[1]])

```

The first row of the data column is just a data frame containg data for all countries in Africa. Here we have year, lifeExp, pop, and gdpPercap. Now we need to make a function which we can apply across these tibbles, which take the same variable names (for sake of ease)

### Mapping

```{r mapfunc}
cont_mod <- function(df){
  mod1 <- lm(lifeExp ~ year + gdp1000 + I(gdp1000**2), data = df)
  return(mod1)
}
```


Now that we have our model function, we can map the model onto our nested data frame and make a new variable called `model`

```{r mapmodel}
nest_df_mod <- nest_df %>%
  mutate(model = map(data, cont_mod))

```

We've mapped our model. Let's see what one of the models looks like

```{r revealmod}
summary(nest_df_mod$model[[1]])
```

Here we have a model of all of the African countries predicting life expectancy. We can continue using map to get a tidy model output, augment our output, or look at fit statistics with glance

### Mapping broom functions

```{r TidyAugGlance}
nest_df_fortify <- nest_df_mod %>%
  mutate(tdy = map(model, tidy, conf.int = T),
         aug = map(model, augment),
         glnc = map(model, glance))

head(nest_df_fortify)
```

### `unnest`

You might be thinking "This is great! But how the hell do we work with this now?" That tripped me up for a while too, but the easiest thing for us to do is something called `unnest`. It takes a nested column, and expands it back to original tibble size. I'll demonstrate.

```{r UnnestTidy}

df_tidy <- nest_df_fortify %>%
  select(continent, tdy) %>%
  unnest(tdy)

head(df_tidy)

df_tidy %>%
  filter(term != '(Intercept)') %>%
  ggplot(aes(x = estimate, y = term, color = continent)) +
  geom_point() +
  geom_errorbar(aes(xmin = conf.low, xmax = conf.high), width = 0.3) +
  facet_wrap(~continent, ncol = 1)


```

Resource: <a href = "http://www.sthda.com/english/wiki/ggplot2-error-bars-quick-start-guide-r-software-and-data-visualization">Tutorial on making error bars</a> from sthda.

Voila! Now we have a data frame of continents with model parameters including standard errors, p values, confidence intervals, etc... This may not be useful for only 5 categories, but imagine if you're making models for 100 categories! Also, purrr implements this type of stuff extremely efficiently, so running models with map is much faster than running 100 different models individually. For demonstration's sake, I'll just show the output of the augment and glance unnests

### Utilizing Glance

```{r UnnestGlance}

df_glance <- nest_df_fortify %>%
  select(continent, glnc) %>%
  unnest(glnc)

print(df_glance)

```

Now we can easily compare fit statistics across countries. We can see that our $R^2$ value for Africa is much lower than the other continents, suggesting that our model did not fit that country well. Perhaps this requires some tuning in the future.

### Utilizing `augment`

```{r UnnestAug}

df_aug <- nest_df_fortify %>%
  select(continent, aug) %>%
  unnest(aug)

# illustration of why plotting a linear regression over a multivariate model
# doesn't work
df_aug %>%
  ggplot(aes(x = year)) +
  geom_point(aes(y = lifeExp), alpha = 0.3) +
  geom_point(aes(y = .fitted), color = 'red', alpha = 0.3) +
  facet_wrap(~continent) +
  labs(title = "Fitted values of LifeExpectancy in gapminder")
  

```

# Stopping point for 2021-03-18

---

# Other Basic Regression Types: glm and glm.nb

R, being a statistical language, has many other types of models that you can implement, most of which can be accessed using the glm function. Let's load in some data that might allow us to use some different distributions

This is just data cleaning and retrieval, so we aren't going to cover this in depth

```{r LoadCOVID}

# rm(list = ls())

cases_global <- read_csv("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_confirmed_global.csv") %>%
  dplyr::select(-Lat, -Long) %>%
  pivot_longer(cols = c(everything(), -`Province/State`, -`Country/Region`),
               names_to = "date",
               values_to = "cases") %>%
  mutate(date = as.Date(date, format = '%m/%d/%y'))

deaths_global <- read_csv("https://raw.githubusercontent.com/CSSEGISandData/COVID-19/master/csse_covid_19_data/csse_covid_19_time_series/time_series_covid19_deaths_global.csv") %>%
  dplyr::select(-Lat, -Long) %>%
  pivot_longer(cols = c(everything(), -`Province/State`, -`Country/Region`),
               names_to = "date",
               values_to = "deaths") %>%
  mutate(date = as.Date(date, format = '%m/%d/%y'))

COVID_master <- cases_global %>%
  left_join(deaths_global, by = c('Province/State', 'Country/Region', 'date')) %>%
  rename(country = 'Country/Region',
         state = 'Province/State') %>%
  group_by(country, date) %>%
  summarise(cases = sum(cases, na.rm = T),
            deaths = sum(deaths, na.rm = T)) %>%
  mutate(daily_cases = ave(cases, country, FUN = function(x) c(0, diff(x))),
         daily_deaths = ave(deaths, country, FUN = function(x) c(0, diff(x))),
         daily_cases_s = ave(daily_cases, country, FUN = function(x) kza::kz(x, m = 14, k = 2)),
         daily_deaths_s = ave(daily_deaths, country, FUN = function(x) kza::kz(x, m = 14, k = 2)))

COVID_master %>%
  ggplot(aes(x = date, y = daily_cases_s, group = country, color = country)) +
  geom_line(alpha = 0.4) +
  theme(legend.position = 'none')

COVID_master %>%
  ggplot(aes(x = date, y = daily_deaths_s, group = country, color = country)) +
  geom_line(alpha = 0.4) +
  theme(legend.position = 'none')
```

Let's look at the Netherlands!

```{r Netherlands}

COVID_Ned <- COVID_master %>%
  filter(country == "Netherlands")

COVID_Ned %>%
  ggplot(aes(x = date)) +
  geom_line(aes(y = daily_cases), color = 'black') +
  geom_line(aes(y = daily_deaths), color = 'black') +
  geom_line(aes(y = daily_cases_s), color = 'blue') +
  geom_line(aes(y = daily_deaths_s), color = 'firebrick3')

COVID_Ned %>%
  pivot_longer(cols = c('daily_cases', 'daily_deaths'),
               names_to = 'outcomes',
               values_to = 'values') %>%
  ggplot() +
  geom_histogram(aes(x = values)) +
  facet_wrap(~outcomes)

mean(COVID_Ned$daily_cases)
var(COVID_Ned$daily_cases)
mean(COVID_Ned$daily_deaths)
var(COVID_Ned$daily_deaths)
```

Here we see that the mean value of cases and deaths is much less than the variance. In addition, both data are highly skewed. We could either use a transformation to normalize these data, or use a glm distribution that accomodates these factors, such as poisson or negative binomial distributions. We can do this using the `glm()` function. Let's model how COVID rates change by the weekday

```{r WeekdayPoisson}
COVID_Ned_wk <- COVID_Ned %>%
  mutate(DoW = weekdays(date))

pmod <- glm(daily_cases ~ date +DoW + daily_cases_s + I(daily_cases_s**2), family = 'poisson', data = COVID_Ned_wk)
summary(pmod)
```

We have model output, but it's difficult to interpret poisson regression coefficients without exponentiating them. Let's bring tidy back into the mix

```{r tidypoiss}
tidy(pmod, conf.int = TRUE, exponentiate = T)

```

```{r}
augment(pmod) %>%
  ggplot(aes(x = date)) +
  geom_point(aes(y = daily_cases)) +
  geom_line(aes(y = exp(.fitted)), color = 'red')
```

### Negative Binomial glm

Negative binomial regression models are generalizations of poisson-distributed models, where an added parameter for overdispertion is included in the model. In a poisson-distribution, the variance is considered to be equal to the mean of the distribution. This is therefore a model assumption of a poisson model of counts. Sometimes, however, our data does not fit this frame of variance being equal to the mean. In this case, we must estimate the overdispersion with a dispersion parameter, which is accounted for with the `glm.nb` function from the MASS package. First, let's calculate the mean and variation of the distribution of cases in our dataset
```{r VMratio}
m <- mean(COVID_Ned_wk$daily_cases)
v <- var(COVID_Ned_wk$daily_cases)

print(paste0("The mean value of daily_cases is ", m))
print(paste0("The variance of daily_cases is ", v))
print(paste0("The ratio of variance:mean is ", v/m))



```

Here we can see that the variance is much larger than the mean of daily cases. We can get more specific and look at the ratio of mean to variance on a monthly basis, which might make more sense than looking at the entire time series together

```{r OvDispMonths}
COVID_Ned_wk %>%
  mutate(month = months(date)) %>%
  group_by(month) %>%
  summarise(mean = mean(daily_cases, na.rm = T),
            var = var(daily_cases, na.rm = T),
            VMratio = var/mean)
```

Above we can see that throughout all months,  the ratio of variance to mean is extremely high, showing that overdispersion is characteristic of the entire time series, not just one or two segments. Therefore it would make sense to apply a negative binomial generalized linear model (which is extremely easy to do)

```{r}
library(MASS)
NBmod <- glm.nb(daily_cases ~ date +DoW + daily_cases_s + I(daily_cases_s**2), data = COVID_Ned_wk)
tdyNBmod <- tidy(NBmod, conf.int = T, exponentiate = T)
augNBmod <- augment(NBmod)
gln <- glance(NBmod)
```

below, we can plot the fitted values of our model to the data. As you can see, the fit looks exactly the same as the poisson model, but the standard errors and confidence intervals should be different.

```{r}

plot(x = augNBmod$date, y = augNBmod$daily_cases)
lines(x = augNBmod$date, y = exp(augNBmod$.fitted), col = 'red', lwd = 2)
lines(x = augNBmod$date, y = confint)

```

our model fit isn't half bad... Even so, there are portions of the model that don't seem to fit quite right, and it would be interesting to see the biases in our model fit, and if there are areas that can be improved. Unfortunately, support for glm.nb models isn't as fully developed as for linear models, so we have to do some of the grunt work ourselves. The .resid component of the augment output for glm.nb models is somewhat unusable, so we can make our own residuals by taking the difference between our original data and the exponent of our fitted values.

```{r}
augNBmod_fit <- augNBmod %>%
  mutate(.resid = daily_cases - exp(.fitted))

augNBmod_fit %>%
  ggplot(aes(x = date, y = .resid)) +
  geom_point() +
  geom_hline(yintercept = 0)
  
```

As we can see, it looks like the residuals are still heavily autocorrelated. R has some cool functions for time series that lets you see the extent of this correlation. One is called `acf()` which just calculates an autocorrelation function. Here we can calculate both the autocorrelation of our residuals and the autocorrelation of our original plot

```{r}
acf(augNBmod_fit$.resid, lag.max = 40)
acf(augNBmod_fit$daily_cases, lag.max = 40)
```

We see above that there is obvious autocorrelation in our residuals in addition to our original plot, as we would expect with a time series This somewhat throws a wrench in our analysis...


## Logistic regressions

Logistic regressions are implemented very simply with the `glm()` function, specifying the argument `family = 'binomial'`. I'll adapt the gapminder dataset below to demonstrate

```{r}
df <- gapminder %>%
  mutate(lifeExpOv75 = ifelse(lifeExp > 75, 1, 0))

# now we can use lifeExpOv75 as our logistic regression outcome variable
logmod <- glm(lifeExpOv75 ~ year + I(gdpPercap/1000) + continent, family = 'binomial', data = df)

tidy(logmod, exponentiate = T, conf.int = T) %>%
  mutate(across(where(is.numeric), round, 2))

```

here we see in our tidy output that every year increase is significantly associated with 1.14 times the odds of having a life expectancy over 75. Every $1000 increase in gdpPercap is associated with 1.15 times the odds of having a life expectancy over 75. In addition, we can see that Africa is not shown in the model output. This is because Africa serves as the reference country in our model. If we wanted to change that, we would have to relevel the continent factor. We can see that though our confidence intervals are extremely wide, they are still statistically significant. However, if we chose a different reference level, we may be able to have more interprable confidence intervals. 

One final thing, we want to mention that intercepts are not interprable in this model, as the intercepts represent life expectancy at year 0, where gdpPercapita is 0, and the continent is Africa. If we wanted interprable intercepts, we would have to mean-standardize all of our variables in our model.


Note: This tutorial is very similar to the tutorial in R for data science by Hadley Wickham and Garret Grolemund in their "Many Models" chapter (Chapter 25). I do not take credit for the ideas presented here, as this tutorial is an adaptation of their tutorial, using similar data. For those looking for a more formal introduction to building many models, and the mechanics behind it, I'll leave a link to their book chapter <a href = "https://r4ds.had.co.nz/many-models.html">here</a>. 

